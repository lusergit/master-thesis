\chapter*{Introduction}\label{ch:intro}
\markboth{INTRODUCTION}{INTRODUCTION}
\addcontentsline{toc}{chapter}{Introduction}
% \section{Introduction and related work}\label{sec:prev}

Because of its widespread adoption software has become a crucial
aspect of everyone's life for all sorts of tasks, from the more
mundane ones -- like sending text messages or view online content --
to the most crucial ones. Banking, aviation, space industry, car
controls are only a small example of important everyday tasks that
software runs in the modern era. Such tasks demand requirements of
safety and reliability which are difficult to pair with the growing
complexity and size of contemporary software. Errors can be expensive
both in monetary and in human lives terms, hence preventing them
becomes more and more valuable as well as detecting them early.

\medskip

Notable examples of such bugs are Meltdown and
Spectre~\cite{art:meltdown,art:spectre}. Those vulnerabilities
exploited an hardware related bug in floating-point division to access
data outside the bounds imposed to a program by the operating system,
resulting in the theft of arbitrary data, meaning a malicious actor
could access -- for example -- passwords stored locally or data of
other customers in a cloud environment.  Another notable example is
the first internet worm, which allowed the deployer to run arbitrary
code on a significant portion of the computers on the internet at the
time~\cite{art:worm1,art:worm2,art:worm3,art:worm4}.  The last example
is set on 4 June 1996, when the Ariane 501 satellite launch failed
catastrophically 40 seconds after initiation of the flight sequence,
incurring a direct cost of approximately 370 million US
dollars~\cite{10.1145/251880.251992}. To assess the causes of the
incident, the automated analysis of the Ariane
code~\cite{art:arianecode} was done using a static analyzer based on
Abstract Interpretation~\cite{art:arianeabstract}.

\medskip

\emph{Software verification} is therefore a crucial task, which cannot
be accomplished using testing practices alone: testing in fact can be
used to show the \emph{presence} of bugs (if a test fails the bug
occur), but they do not offer any \emph{mathematical guarantee} of
their absence. The latter can be obtained trough \emph{formal
  methods}, i.e., by mathematical proving the correctness of a program
with respect to some \emph{specification}.

\paragraph*{Formal methods.} Despite the progress done to bring the
usefulness of formal methods to everyone (e.g.\
with~\cite{10.1145/3371078, 10.1145/3338112} or with the Grand
Challenge of software verification~\cite{1621009,Hoare2008,1707636})
their use is still restricted to specific niches of developers. This
is due to some problems with the technique itself. Firstly, the problem
is intrinsic in the theory of computation. Consider the following
program in a pseudo-C language

\begin{lstlisting}[language=C,escapechar=|,]
  int* p = NULL;
  arbitrary_function();
  *p = 0;|\label{line:crash}|
\end{lstlisting}

If control reaches Line~\ref{line:crash} the program will crash (as we
are trying to access address \(\mathsf{0x0}\)). Hence we have to
prove that \texttt{arbitrary\_function()} does not halt. Unfortunately
Turing~\cite{10.7551/mitpress/12274.003.0008} shows that this problem
is undecidable. Moreover, Rice's Theorem~\cite{rice1953classes}
expands on this stating that \emph{all non-trivial semantics
  properties} of programs are undecidable. The consequence is that we
cannot have an \emph{universal verifier}, i.e., a verification tool
that proves or disproves the correctness of every program with respect
to some specification. However we do not need to solve such a general
problem. We as humans tend to use patterns and structures, even to
write our logic. The outcome is that we work with just a small subset
of all possible programs, and therefore to work in practice our
analyzers have to trace the correctness of just that small subset of
programs.

\medskip

Moreover the result of our analysis does not have to be the most
precise description of the outcome of the program we are analyzing. We
need a tool that can state that our program satisfies a property (an
\emph{invariant}) which is \emph{sound} to the real property of the
program, i.e., a property which is \emph{less precise} than the real
one. Notable example of such analyzers are well known and available
today on the internet. For example
Astre√®~\cite{10.1007/978-3-540-31987-0_3} and
Mopsa~\cite{10.1007/978-3-031-30820-8_37} are two sound analyzers of C
and python code, which can infer program properties and catch bugs
ahead of time. They both use a technique called \emph{abstract
  interpretation} which (roughly) involves interpreting a given
program by mapping variables to an abstract representation of some
properties we are interested in.

\paragraph*{Abstract interpretation.} Since universal program
verification is a fundamentally undecidable problem, the best we can
do is to consider non-universal verification, at the cost of getting
non-conclusive answers. In this work we focus on one of the major
technique for software analysis that can be used to implement a sound
verifier: abstract interpretation. To best introduce the technique we
start from an example. Consider the following fragment of pseudo-C
code:

\begin{lstlisting}[language=C, label=codeexample, caption=Incrementing or decrementing randomly]
  int x = 0;
  for(int i = 0; i < 5; i++){
    int val = rand();
    if (val > 0.5) x += 2;
    else x -= 2;
  }
  printf("%d", x);
\end{lstlisting}

Each execution of the snippet could result in a different value of
\texttt{x} being printed on screen. From a mathematical point of view,
before entering the loop the value of \texttt{x} is fixed, it can only
be \(0\). Abstracting the execution means \emph{abstracting} the
values the variables can assume. For this example variables can assume
\emph{interval} values, e.g., \(\var \in \interval{0}{0}\) at the
beginning of the interval. Assuming \texttt{rand()} returns a
\texttt{float} value in \(\interval{0}{1}\), at each iteration either
\(\var\) is incremented or decremented by 2. Hence, after the first
iteration \(\var \in \interval{-2}{+2}\), after the second
\(\var \in \interval{-4}{+4}\), and so on. The loop could carry on
forever, however, because of the \texttt{for} guard \(\var[i] < 5\) we
reach a stall at \(\var \in \interval{-10}{+10}\). Therefore at the end
of the loop, what our analysis can infer is that
\(\var \in \interval{-10}{+10}\).

\medskip

\noindent
The analysis is sound: the most precise property of the value of
\(\var\) would be being in the set
\({S = \{-10, -8, -6, -4, -2, 0, +2, +4, +6, +8, +10\}}\), as for each
iteration the value of \(\var\) can increment or decrement by \(2\),
and our result \(\interval{-10}{+10}\) is a \emph{superset} of \(S\).

\medskip

\emph{Abstract interpretation}, introduced by Radhia and Patrick
Cousot in~\cite{patrickradhia:one, patrickradhia:two} is therefore a
framework that generalizes this idea by providing tools to compute
efficiently general abstractions. As a result, a sound verifier can be
obtained by comparing the resulting abstraction with the program
specification: if the latter satisfies the former then also the
program does and therefore it is correct, otherwise noting can be said
about the program, as the abstraction is an \emph{over-approximation}.

\medskip

\noindent
With the example we already introduced the idea of the framework of
abstract interpretation:
\begin{itemize}
\item We start from a \emph{concrete semantics}, describing the
  meaning of program commands in a \emph{computational domain};
\item We define an \emph{abstract domain}, which models some
  properties of interest of the concrete computation and leaves out
  the details (in our example, the \emph{interval} domain);
\item We induce an \emph{abstract semantics}, based on the concrete
  semantics and our abstract domain, which allows to \emph{abstractly
    execute} our program on the abstract domain in order to compute
  the program properties modeled by the abstract domain;
\item The result of our abstract execution is the final property of
  the program.
\end{itemize}
Generally the abstract execution of our program (i.e.\ the abstract
interpretation) involves \emph{fixpoint computations} on algebraic
structures called \emph{lattices}. Lattices are sets equipped with a
notion of \emph{order} between elements, while fixpoint computations
usually involve computing a minimal element in a \emph{chain} of
lattice elements such that the computation does not proceed further.
In other words, at which point the guard of a loop is not satisfied
anymore and which guarantees can we infer for the program \emph{after}
the loop? As an example, consider the snipped of
Code~\ref{codeexample}. We argued that our computation of the abstract
value of the variable \(\var\) could start from \(\interval{0}{0}\)
and proceed with \(\interval{-2}{+2}, \interval{-4}{+4}, \dots\) In
this case we find a fixpoint with \(\interval{-10}{+10}\), since the
guard of the loop is not respected anymore (because of the variable
\(\var[i]\)) and therefore ``executing'' the loop again would result
in the same initial value \(\interval{-10}{+10}\), hence a
\emph{fixpoint} is reached.

\medskip

\noindent
However this is not always the case. Consider instead the following
snippet
\begin{lstlisting}[language=C, caption=Program with a divergent loop, label=exampleloop]
  int x = 0;
  while(true) {
    x += 1;
  }
\end{lstlisting}
Obviously the concrete computation does not halt, but looking at the
chain of iterands for our program for the variable \(\var\) we could
infer the chain
\begin{equation}\label{eq:diverging}
  \interval{0}{0}, \interval{+1}{+1}, \dots, \interval{+k}{+k}, \dots
  \tag{\dag}
\end{equation}
and therefore (intuitively) at the end of the loop we might want to
say that our variable has a value in the range
\(\interval{0}{+\infty}\), which is sound to the \emph{real} property
of non-termination (i.e., \(\var\in\emptyset\), \(\var\) has \emph{no}
final value). The procedure we used to infer that the variable
\(\var\) diverges, i.e., its invariant at the end of the loop is
\(\interval{0}{+\infty}\), comes from intuition, and is not an
automatic procedure. What a machine could do in fact is exploring the
chain we are building in~\eqref{eq:diverging} element by element, and
if at some point of the chain an element is repeated it can infer a
fixpoint.  What happens with this naive method is that whenever an
\emph{infinite} ascending (or descenging) chain is involved the
termination of the analysis is not guaranteed.

To infer properties while dealing with infinite chains the standard
approach is to use \emph{widening} operators
(from~\cite{10.1007/3-540-55844-6_142}, usually denoted as \(\widen\))
to infer the \emph{divergence} of a variable after some round of
increments. Such technique however, while still providing a sound
analysis and ensuring termination, limits a lot the precision of the
analysis itself. For example, we could widen our analysis after the
second step with a naive widening
\(\interval{0}{0} \widen \interval{+1}{+1} = \interval{0}{+\infty}\)
and infer that our variable after the end of the loop has a value
between \(0\) and \(+\infty\) while ensuring that our analysis halts
in a finite time.  It might happen however that the widening loses
more information than needed to ensure termination. Consider in fact
the following snippet

\begin{lstlisting}[language=C, caption=Program with a convergent loop,
  label=exampleloopconverges]
  int x = 0;
  while(x < 10) {
    x += 1;
  }
\end{lstlisting}

The initial steps of the analysis would still be
\(\var \in \interval{0}{0}\) and \(\var \in \interval{1}{1}\), which
widen become again
\(\interval{0}{0}\widen\interval{1}{1} = \interval{0}{+\infty}\),
which again is a sound analysis but is very imprecise. Simply by
looking at the program we can infer that the cycle converges and at
the end of the loop \(\var\in\interval{0}{10}\). To recover simple
information as this from the program~\cite{10.1007/3-540-55844-6_142}
also introduce a \emph{narrowing} operator, denoted as \(\narrowi\).
This combination however still has some limitation, as it is heavely
relying on the information directly embedded in the program.  This
opens a question, which is the general problem faced in the thesis:

\begin{problem*}
  Is it possible to have a precise analysis of abstract properties,
  while ensuring the termination of the analyzer?
\end{problem*}

The thesis starts by formalising some expected results about the
undecidability of the collecting semantics for a simple imperative,
Turing complete language. The language include a minimal set of
constructs that make it Turing powerful: linear assignements, interval
based guards, non-deterministic choice (which jointly with guards
allows us to encode conditionals) and a fixpoint operation (which can
be used to encode while loops). Roughly speaking, what is shown is
that precise invariants cannot be computed. Actually, it is not even
possible to decide the finiteness of invariants, as the halting
problem, which is notoriously undecidable, reduces to the finiteness
of program invariants.

We try to provide some answers to the question above, by focusing on
so-called non-relational abstract semantics, i.e., abstract semantics
which disregard the depedencies between the different variables in a
program and provide, for each single variable, an abstraction of the
set of possible values. It must be observed that if in the abstract
domain of interest all ascending chains are finite, the problem
trivialises as all fixpoint computations will converge in a finite
amount of steps. Thus they can be conducted in a precise way, without
the need of any additional theory. The thesis will start by focusing
on the domain of intervals, where as observed above one can have
infinite ascending chaings and thus the computation of the exact
abstract semantics is a challenging task. Actually, in principle, one
might even think that this is not possible because, in absence of a
result proving the converse, establishing whether the abstract
analyser terminates (which reduces to establish whetehr ordinary
fixpoint iteration trough Kleene iteration does terminate) might be
undecidable.

Actually, some results showing that, under some restrictions, interval
analysis can be computed exactly were already
available. In~\cite{SU2005122} the autors identify a class of interval
equations for which the least solution can be computed precisely in
polynomial time and the result is later generalised
in~\cite{Gawlitza2009} which provides an algorithm that also deals
with the arbitrary multiplication of intervals.

In this thesis we take a radically different approach. We show that a
preliminary analysis of the program allows one to identify some bounds
such that, intuitively, when variables are beyond the bounds the
behaviour of the program stabilises. This is the basic tool which
allows us to determine whether fixpoint computations will diverge,
thus providing a way of estalishing the exact result of the
computation. Technically, this is obtained by restricting the abstract
domain to intervals which are bounded or infinite. Then we prove that
the interval semantics computed over this subdomain coincide with the
semantics computed on the full domain. This, together with the fact
that the bounded domain has no infinite ascending chain allows us to
conclude.

The generality of the approach suggests the possibilty of extending
the result to other non-relational semantics. We then focus on what we
call the non-relational collecting semantics, the most precise
non-relational semantics, which maps each variable to a set of
possible values, forgetting only the relation between variables. The
fact that invariants are no longer convex sets, makes the problem
sensibly more difficult. We do not succeed in showing that the
abstract semantics is computable, but we manage to prove a partial
result which, intuitively, says that the termination of the abstract
interpreter is decidable or, equivalently, that all inviariants
generated during the analysis of the program are finite. In case the
program has a single while loop, this corresponds exactly to the
finiteness of the invariants. This partial results makes us confident
about the possibility of proving that also the non-relational
collecting semantics is computable in an exact way.

\paragraph*{Outline}
The following document consists of 5
chapters. Chapter~\ref{ch:background} provides the necessary
background and fixes the necessary notation that will later be used in
the following chapters: from recursion and order theory to talk about
program termination and undecidability to abstract interpretation to
prove some properties of our analysis. Chapter~\ref{ch:framework}
introduces the framework of the thesis: the \(\imp\) language and its
constraints, its \emph{concrete} semantics and its properties: namely
undecidability of some properties of programs written in the
language. Chapter~\ref{ch:abstractdomains} proves the properties of
interval and non-relational collecting analysis on the \(\imp\)
language, while Chapter~\ref{ch:axiomatized} proves that, similarly to
previous work, it is possible to bound the domains we previously
introduced to remove infinite ascending chains, hence ensuring
analysis termination. Finally Chapter~\ref{ch:conclusion} presents the
thesis results and proposes a direction for future work on the topic.
